{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/umar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Optional\n",
    "import feather\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from dateutil.parser import isoparse\n",
    "from hashlib import sha1\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "import string\n",
    "import re\n",
    "from nltk.stem.porter import *\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize utterances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Davidson's custom tokenizer\n",
    "stemmer = PorterStemmer()\n",
    "def tokenize(tweet):\n",
    "    \"\"\"Removes punctuation & excess whitespace, sets to lowercase,\n",
    "    and stems tweets. Returns a list of stemmed tokens.\"\"\"\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z]*\", tweet.lower())).strip()\n",
    "    #tokens = re.split(\"[^a-zA-Z]*\", tweet.lower())\n",
    "    tokens = [stemmer.stem(t) for t in tweet.split()]\n",
    "    return tokens\n",
    "\n",
    "# Function to gather and tally hateful n-grams found in a given article and calculate the \"hatescore\" for that article\n",
    "def tally_counts_doc(row):\n",
    "    row2 = row[row > 0]\n",
    "    score = 0\n",
    "    hits = {}\n",
    "    for index,val in row2.items():\n",
    "        hits[index]=val\n",
    "        hit = dict_hateweights[index] * val\n",
    "        score += hit \n",
    "    row['hate_score'] = score\n",
    "    row['hate_hits'] = hits\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __________________Load DF of Hateful terms\n",
    "df_hate_words = pd.read_csv('data/raw_other/hateful_ngrams.csv')\n",
    "df_hate_words.set_index('ngram',drop=True,inplace=True)\n",
    "dict_hateweights = df_hate_words['prophate'].to_dict()\n",
    "hate_list = list(df_hate_words.index)\n",
    "\n",
    "#____________________ Load text to vectorize\n",
    "\n",
    "cols = ['conv_id',  'timestamp', 'source', 'text']\n",
    "df_utters = pd.read_csv('out/article_data_df_1_1.ftr.csv',names=cols)\n",
    "df_utters = df_utters[df_utters['source'] == 'client']\n",
    "\n",
    "df_utters = df_utters.head(24000) #-------------------------------abbrveiate sample\n",
    "df_utters.reset_index(inplace=True,drop=True)\n",
    "\n",
    "#____________________instantiate and fit the vectorizer\n",
    "unlab_cvect = CountVectorizer(\n",
    "    ngram_range=(1,4),\n",
    "    vocabulary = list(df_hate_words.index)\n",
    ")\n",
    "\n",
    "\n",
    "unlab_vectors = unlab_cvect.fit(df_utters['text'])\n",
    "unlab_matrix = unlab_vectors.transform(df_utters['text'])\n",
    "df_vectors = pd.DataFrame(unlab_matrix.todense(),columns=unlab_vectors.get_feature_names())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_utters.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tally hate score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#______________________________________Gather hate counts and calculate hate scores\n",
    "tqdm.pandas(desc=\"counting hate score\")\n",
    "\n",
    "df_vectors = df_vectors.progress_apply(lambda row:tally_counts_doc(row),axis=1,result_type='expand')\n",
    "\n",
    "df_hatecounts = pd.concat([df_utters,df_vectors],axis=1)\n",
    "\n",
    "df_hatecounts['hate_hits'] = df_hatecounts['hate_hits'].astype('string')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hatecounts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hatecounts = df_hatecounts[df_hatecounts['hate_score'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hate_counts = df_hatecounts.sort_values('hate_score',ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hate_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts = df_vectors.sum(axis=0)\n",
    "df_counts.sort_values(inplace=True,ascending = False)\n",
    "df_counts = df_counts[df_counts > 0]\n",
    "df_counts.plot(kind='barh',figsize=(10,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "import pyspark\n",
    "import random\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType, IntegerType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import NGram\n",
    "findspark.init()\n",
    "\n",
    "sc.stop()\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "cols = ['conv_id',  'timestamp', 'source', 'text']\n",
    "df_utters = spark.read.csv('out/article_data_df_1_1.ftr.csv')\n",
    "df_utters = df_utters.selectExpr(\"_c0 as conv_id\",\"_c1 as timestamp\",\"_c2 as source\",\"_c3 as text\")\n",
    "\n",
    "df_utters = df_utters.limit(5000)\n",
    "\n",
    "#___________________________  Tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "def tokenize(tweet):\n",
    "    \"\"\"Removes punctuation & excess whitespace, sets to lowercase,\n",
    "    and stems tweets. Returns a list of stemmed tokens.\"\"\"\n",
    "    #tweet = \" \".join(re.split(\"[^a-zA-Z]*\", tweet.lower())).strip()\n",
    "    tokens = re.split(\"[^a-zA-Z]*\", tweet.lower().strip())\n",
    "    tokens = [stemmer.stem(t) for t in tweet.split()]\n",
    "    return tokens\n",
    "\n",
    "token_udf = udf(tokenize,ArrayType(StringType()))\n",
    "#df_utters  = df_utters.withColumn(\"tokens\", token_udf(F.col('text')))\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "df_utters = tokenizer.transform(df_utters)\n",
    "#____________________________ n-grams\n",
    "\n",
    "\n",
    "for i in range(1,5):\n",
    "    \n",
    "    ngram = NGram(n=i,inputCol=\"tokens\",outputCol='n' + str(i))\n",
    "\n",
    "    df_utters = ngram.transform(df_utters)\n",
    "    \n",
    "add_lists = F.udf(lambda a,b,c,d : a + b + c + d,ArrayType(StringType()))\n",
    "df_utters = df_utters.withColumn('n_all',add_lists('n1','n2','n3','n4'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, FloatType, StringType, IntegerType\n",
    "\n",
    "col_len = F.udf(lambda a: len(a),IntegerType())\n",
    "\n",
    "df_utters = df_utters.withColumn('token_len',col_len('tokens'))\n",
    "df_utters = df_utters.withColumn('n_all_len',col_len('n_all'))\n",
    "\n",
    "for i in range(2,5):\n",
    "    df_utters = df_utters.withColumn('n'+ str(i) + '_len',col_len('n'+ str(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_utters.where(df_utters['n_all_len'] <= 14).select('n_all_len').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "# __________________Load DF of Hateful terms\n",
    "df_hate_words = pd.read_csv('data/raw_other/hateful_ngrams.csv')\n",
    "df_hate_words.set_index('ngram',drop=True,inplace=True)\n",
    "dict_hateweights = df_hate_words['prophate'].to_dict()\n",
    "hate_list = list(df_hate_words.index)\n",
    "\n",
    "df_utters = df_utters.na.drop(\"all\")\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"n_all\", outputCol=\"features\")\n",
    "\n",
    "cvec_model = cv.fit(df_utters)\n",
    "cvec_model.transform(df_utters).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "216px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
