{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                     conv_id      timestamp  source  \\\n",
       "24  ec12f8e13303cfee07733536855588e61f88123a  1593182152000  client   \n",
       "31  24d83edae09283e3dcf22ab19df4ba2f69c2dfa7              0  client   \n",
       "9   99c5218bc02b270a71db4646255c1daf4bfdf8f7  1602852120000  client   \n",
       "25  6d59a7e07384968356eeb67b26978f3f5f02cab7  1602071781000  expert   \n",
       "14  3c3c8fbc3710d908d50fb336afaa045ecf73ed43  1602641036000  client   \n",
       "\n",
       "                                                 text  hate_score  \\\n",
       "24  18-year-old Althea Bernstein — who is black or...       1.112   \n",
       "31  For centuries following the Dorian Invasion, t...       0.667   \n",
       "9   'Worrisome' discovery | Climate change wiped o...       0.667   \n",
       "25  Subtitle: More than 500 years after launching ...       0.667   \n",
       "14  “You either turn yourself in, kill yourself or...       0.667   \n",
       "\n",
       "               hate_hits  \n",
       "24      {'white boy': 2}  \n",
       "31          {'slave': 1}  \n",
       "9            {'homo': 1}  \n",
       "25          {'slave': 1}  \n",
       "14  {'kill yourself': 1}  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>conv_id</th>\n      <th>timestamp</th>\n      <th>source</th>\n      <th>text</th>\n      <th>hate_score</th>\n      <th>hate_hits</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>24</th>\n      <td>ec12f8e13303cfee07733536855588e61f88123a</td>\n      <td>1593182152000</td>\n      <td>client</td>\n      <td>18-year-old Althea Bernstein — who is black or...</td>\n      <td>1.112</td>\n      <td>{'white boy': 2}</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>24d83edae09283e3dcf22ab19df4ba2f69c2dfa7</td>\n      <td>0</td>\n      <td>client</td>\n      <td>For centuries following the Dorian Invasion, t...</td>\n      <td>0.667</td>\n      <td>{'slave': 1}</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>99c5218bc02b270a71db4646255c1daf4bfdf8f7</td>\n      <td>1602852120000</td>\n      <td>client</td>\n      <td>'Worrisome' discovery | Climate change wiped o...</td>\n      <td>0.667</td>\n      <td>{'homo': 1}</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>6d59a7e07384968356eeb67b26978f3f5f02cab7</td>\n      <td>1602071781000</td>\n      <td>expert</td>\n      <td>Subtitle: More than 500 years after launching ...</td>\n      <td>0.667</td>\n      <td>{'slave': 1}</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>3c3c8fbc3710d908d50fb336afaa045ecf73ed43</td>\n      <td>1602641036000</td>\n      <td>client</td>\n      <td>“You either turn yourself in, kill yourself or...</td>\n      <td>0.667</td>\n      <td>{'kill yourself': 1}</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "df_utters = pd.read_parquet('data/venv3-00000-of-00001')\n",
    "df_utters =df_utters.sort_values('hate_score', ascending = False)\n",
    "df_utters.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hate_words = pd.read_csv('data/raw_other/hateful_ngrams.csv')\n",
    "df_hate_words.set_index('ngram',drop=True,inplace=True)\n",
    "dict_hateweights = df_hate_words['prophate'].to_dict()\n",
    "hate_list = list(df_hate_words.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'allah akbar': 0.87,\n",
       " 'blacks': 0.583,\n",
       " 'chink': 0.467,\n",
       " 'chinks': 0.542,\n",
       " 'dykes': 0.602,\n",
       " 'faggot': 0.489,\n",
       " 'faggots': 0.675,\n",
       " 'fags': 0.543,\n",
       " 'homo': 0.667,\n",
       " 'inbred': 0.583,\n",
       " 'nigger': 0.584,\n",
       " 'niggers': 0.672,\n",
       " 'queers': 0.5,\n",
       " 'raped': 0.717,\n",
       " 'savages': 0.778,\n",
       " 'slave': 0.667,\n",
       " 'spic': 0.75,\n",
       " 'wetback': 0.667,\n",
       " 'wetbacks': 0.688,\n",
       " 'whites': 0.556,\n",
       " 'a dirty': 0.743,\n",
       " 'a nigger': 0.509,\n",
       " 'all niggers': 0.859,\n",
       " 'all white': 0.75,\n",
       " 'always fuck': 0.556,\n",
       " 'ass white': 0.542,\n",
       " 'be killed': 0.571,\n",
       " 'beat him': 0.575,\n",
       " 'biggest faggot': 0.667,\n",
       " 'blame the': 0.533,\n",
       " 'butt ugly': 0.5,\n",
       " 'chink eyed': 0.667,\n",
       " 'chinks in': 0.556,\n",
       " 'coon shit': 0.667,\n",
       " 'dumb monkey': 0.667,\n",
       " 'dumb nigger': 0.667,\n",
       " 'fag and': 0.462,\n",
       " 'fag but': 0.556,\n",
       " 'faggot a': 0.667,\n",
       " 'faggot and': 0.556,\n",
       " 'faggot ass': 0.771,\n",
       " 'faggot bitch': 0.667,\n",
       " 'faggot for': 0.533,\n",
       " 'faggot smh': 0.5,\n",
       " 'faggot that': 0.583,\n",
       " 'faggots and': 0.667,\n",
       " 'faggots like': 0.901,\n",
       " 'faggots usually': 0.778,\n",
       " 'faggots who': 0.5,\n",
       " 'fags are': 0.667,\n",
       " 'fuckin faggot': 0.667,\n",
       " 'fucking faggot': 0.636,\n",
       " 'fucking gay': 0.5,\n",
       " 'fucking hate': 0.685,\n",
       " 'fucking nigger': 0.8,\n",
       " 'fucking queer': 0.778,\n",
       " 'gay ass': 0.481,\n",
       " 'get raped': 0.788,\n",
       " 'hate all': 0.556,\n",
       " 'hate faggots': 0.912,\n",
       " 'hate fat': 0.667,\n",
       " 'hate you': 0.663,\n",
       " 'here faggot': 0.571,\n",
       " 'is white': 0.667,\n",
       " 'jungle bunny': 0.667,\n",
       " 'kill all': 0.667,\n",
       " 'kill yourself': 0.667,\n",
       " 'little faggot': 0.667,\n",
       " 'many niggers': 0.708,\n",
       " 'married to': 0.533,\n",
       " 'me faggot': 0.5,\n",
       " 'my coon': 0.6,\n",
       " 'nigga ask': 0.556,\n",
       " 'niggas like': 0.556,\n",
       " 'nigger ass': 0.667,\n",
       " 'nigger is': 0.6,\n",
       " 'nigger music': 0.75,\n",
       " 'niggers are': 0.633,\n",
       " 'of fags': 0.69,\n",
       " 'of white': 0.588,\n",
       " 'raped and': 0.556,\n",
       " 'raped by': 0.763,\n",
       " 'sand nigger': 0.667,\n",
       " 'savages that': 0.667,\n",
       " 'shorty bitch': 0.583,\n",
       " 'spear chucker': 0.667,\n",
       " 'spic cop': 0.778,\n",
       " 'stupid nigger': 0.667,\n",
       " 'that fag': 0.5,\n",
       " 'that faggot': 0.542,\n",
       " 'that nigger': 0.6,\n",
       " 'the faggots': 0.667,\n",
       " 'the female': 0.642,\n",
       " 'the niggers': 0.778,\n",
       " 'their heads': 0.6,\n",
       " 'them white': 0.5,\n",
       " 'then faggot': 0.5,\n",
       " 'this nigger': 0.889,\n",
       " 'to rape': 0.667,\n",
       " 'trailer park': 0.5,\n",
       " 'trash with': 0.667,\n",
       " 'u fuckin': 0.5,\n",
       " 'ugly dyke': 0.667,\n",
       " 'up nigger': 0.77,\n",
       " 'white ass': 0.556,\n",
       " 'white boy': 0.556,\n",
       " 'white person': 0.583,\n",
       " 'white trash': 0.507,\n",
       " 'with niggas': 0.5,\n",
       " 'you fag': 0.611,\n",
       " 'you nigger': 0.827,\n",
       " 'you niggers': 0.755,\n",
       " 'your faggot': 0.667,\n",
       " 'your nigger': 0.556,\n",
       " 'a bitch made': 0.533,\n",
       " 'a fag and': 0.6,\n",
       " 'a fag but': 0.556,\n",
       " 'a faggot and': 0.583,\n",
       " 'a faggot for': 0.583,\n",
       " 'a fucking queer': 0.778,\n",
       " 'a nigga ask': 0.556,\n",
       " 'a white person': 0.778,\n",
       " 'a white trash': 0.6,\n",
       " 'all these fucking': 0.667,\n",
       " 'are all white': 0.75,\n",
       " 'be killed for': 0.571,\n",
       " 'bitch made nigga': 0.533,\n",
       " 'faggots like you': 0.905,\n",
       " 'faggots usually have': 0.778,\n",
       " 'fuck outta here': 0.778,\n",
       " 'fuck u talking': 0.667,\n",
       " 'fuck you too': 0.556,\n",
       " 'fucking hate you': 0.725,\n",
       " 'full of white': 0.792,\n",
       " 'him a nigga': 0.556,\n",
       " 'his shorty bitch': 0.667,\n",
       " 'how many niggers': 0.75,\n",
       " 'is a fag': 0.75,\n",
       " 'is a faggot': 0.476,\n",
       " 'is a fuckin': 0.667,\n",
       " 'is a fucking': 0.799,\n",
       " 'is a nigger': 0.556,\n",
       " 'like a faggot': 0.632,\n",
       " 'like da colored': 0.556,\n",
       " 'many niggers are': 0.714,\n",
       " 'nigga and his': 0.556,\n",
       " 'niggers are in': 0.714,\n",
       " 'of white trash': 0.6,\n",
       " 'shut up nigger': 0.785,\n",
       " 'still a faggot': 0.583,\n",
       " 'the biggest faggot': 0.556,\n",
       " 'the faggots who': 0.556,\n",
       " 'the fuck do': 0.556,\n",
       " 'they all look': 0.778,\n",
       " 'what a fag': 0.556,\n",
       " 'white bitch in': 0.556,\n",
       " 'white trash and': 0.667,\n",
       " 'you a fag': 0.778,\n",
       " 'you a lame': 0.467,\n",
       " 'you a nigger': 0.467,\n",
       " 'you fuck wit': 0.556,\n",
       " 'you fucking faggot': 0.583,\n",
       " 'your a cunt': 0.667,\n",
       " 'your a dirty': 0.87,\n",
       " 'your bitch in': 0.467,\n",
       " 'a bitch made nigga': 0.533,\n",
       " 'a lame nigga you': 0.556,\n",
       " 'faggot if you ever': 0.467,\n",
       " 'full of white trash': 0.867,\n",
       " 'how many niggers are': 0.75,\n",
       " 'is full of white': 0.792,\n",
       " 'lame nigga you a': 0.556,\n",
       " 'many niggers are in': 0.714,\n",
       " 'nigga you a lame': 0.556,\n",
       " 'niggers are in my': 0.714,\n",
       " 'wit a lame nigga': 0.556,\n",
       " 'you a lame bitch': 0.556,\n",
       " 'you fuck wit a': 0.556}"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "dict_hateweights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tweet):\n",
    "    \"\"\"Removes punctuation & excess whitespace, sets to lowercase,\n",
    "    and stems tweets. Returns a list of stemmed tokens.\"\"\"\n",
    "    #tweet = \" \".join(re.split(\"[^a-zA-Z]*\", tweet.lower())).strip()\n",
    "    \n",
    "    tokens = re.split(\"[^a-zA-Z]*\", tweet.lower())\n",
    "    tokens = [stemmer.stem(t) for t in tweet.split()]\n",
    "    return tokens\n",
    "    \n",
    "def ngrams(tokens,n):\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    list_gram = [\" \".join(ngram) for ngram in ngrams]\n",
    "    return list_gram\n",
    "\n",
    "master = []\n",
    "tokens = tokenize(text)\n",
    "\n",
    "for n in range(2,4):\n",
    "    current = ngrams(tokens,n)\n",
    "    master += current\n",
    "\n",
    "master += tokens\n",
    "\n",
    "\n",
    "hate_score = 0\n",
    "hate_hits = {}\n",
    "\n",
    "for gram in master:\n",
    "    if gram in dict_hateweights.keys():\n",
    "        if gram in hate_hits:\n",
    "            hate_hits[gram] += 1 \n",
    "        else:\n",
    "            hate_hits[gram] = 1\n",
    "        hate_score += dict_hateweights[gram]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hate_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize utterances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Davidson's custom tokenizer\n",
    "stemmer = PorterStemmer()\n",
    "def tokenize(tweet):\n",
    "    \"\"\"Removes punctuation & excess whitespace, sets to lowercase,\n",
    "    and stems tweets. Returns a list of stemmed tokens.\"\"\"\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z]*\", tweet.lower())).strip()\n",
    "    #tokens = re.split(\"[^a-zA-Z]*\", tweet.lower())\n",
    "    tokens = [stemmer.stem(t) for t in tweet.split()]\n",
    "    return tokens\n",
    "\n",
    "# Function to gather and tally hateful n-grams found in a given article and calculate the \"hatescore\" for that article\n",
    "def tally_counts_doc(row):\n",
    "    row2 = row[row > 0]\n",
    "    score = 0\n",
    "    hits = {}\n",
    "    for index,val in row2.items():\n",
    "        hits[index]=val\n",
    "        hit = dict_hateweights[index] * val\n",
    "        score += hit \n",
    "    row['hate_score'] = score\n",
    "    row['hate_hits'] = hits\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_utters.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __________________Load DF of Hateful terms\n",
    "df_hate_words = pd.read_csv('data/raw_other/hateful_ngrams.csv')\n",
    "df_hate_words.set_index('ngram',drop=True,inplace=True)\n",
    "dict_hateweights = df_hate_words['prophate'].to_dict()\n",
    "hate_list = list(df_hate_words.index)\n",
    "\n",
    "#____________________ Load text to vectorize\n",
    "\n",
    "cols = ['conv_id',  'timestamp', 'source', 'text']\n",
    "df_utters = pd.read_parquet('out/combined_utterances.parquet',names=cols)\n",
    "#df_utters = df_utters[df_utters['source'] == 'client']\n",
    "\n",
    "df_utters = df_utters.head(12000) #-------------------------------abbrveiate sample\n",
    "df_utters.reset_index(inplace=True,drop=True)\n",
    "\n",
    "#____________________instantiate and fit the vectorizer\n",
    "\n",
    "\n",
    "\n",
    "unlab_vectors = unlab_cvect.fit(df_utters['text'])\n",
    "unlab_matrix = unlab_vectors.transform(df_utters['text'])\n",
    "df_vectors = pd.DataFrame(unlab_matrix.todense(),columns=unlab_vectors.get_feature_names())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tally hate score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#______________________________________Gather hate counts and calculate hate scores\n",
    "\n",
    "df_vectors = df_vectors.apply(lambda row:tally_counts_doc(row),axis=1,result_type='expand')\n",
    "\n",
    "df_hatecounts = pd.concat([df_utters,df_vectors],axis=1)\n",
    "\n",
    "df_hatecounts['hate_hits'] = df_hatecounts['hate_hits'].astype('string')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hatecounts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hatecounts = df_hatecounts[df_hatecounts['hate_score'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hate_counts = df_hatecounts.sort_values('hate_score',ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hate_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts = df_vectors.sum(axis=0)\n",
    "df_counts.sort_values(inplace=True,ascending = False)\n",
    "df_counts = df_counts[df_counts > 0]\n",
    "df_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "import pyspark\n",
    "import random\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType, IntegerType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.feature import NGram\n",
    "findspark.init()\n",
    "\n",
    "sc.stop()\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "cols = ['conv_id',  'timestamp', 'source', 'text']\n",
    "df_utters = spark.read.csv('out/article_data_df_1_1.ftr.csv')\n",
    "df_utters = df_utters.selectExpr(\"_c0 as conv_id\",\"_c1 as timestamp\",\"_c2 as source\",\"_c3 as text\")\n",
    "\n",
    "df_utters = df_utters.limit(5000)\n",
    "\n",
    "#___________________________  Tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "def tokenize(tweet):\n",
    "    \"\"\"Removes punctuation & excess whitespace, sets to lowercase,\n",
    "    and stems tweets. Returns a list of stemmed tokens.\"\"\"\n",
    "    #tweet = \" \".join(re.split(\"[^a-zA-Z]*\", tweet.lower())).strip()\n",
    "    tokens = re.split(\"[^a-zA-Z]*\", tweet.lower().strip())\n",
    "    tokens = [stemmer.stem(t) for t in tweet.split()]\n",
    "    return tokens\n",
    "\n",
    "token_udf = udf(tokenize,ArrayType(StringType()))\n",
    "#df_utters  = df_utters.withColumn(\"tokens\", token_udf(F.col('text')))\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
    "df_utters = tokenizer.transform(df_utters)\n",
    "#____________________________ n-grams\n",
    "\n",
    "\n",
    "for i in range(1,5):\n",
    "    \n",
    "    ngram = NGram(n=i,inputCol=\"tokens\",outputCol='n' + str(i))\n",
    "\n",
    "    df_utters = ngram.transform(df_utters)\n",
    "    \n",
    "add_lists = F.udf(lambda a,b,c,d : a + b + c + d,ArrayType(StringType()))\n",
    "df_utters = df_utters.withColumn('n_all',add_lists('n1','n2','n3','n4'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, FloatType, StringType, IntegerType\n",
    "\n",
    "col_len = F.udf(lambda a: len(a),IntegerType())\n",
    "\n",
    "df_utters = df_utters.withColumn('token_len',col_len('tokens'))\n",
    "df_utters = df_utters.withColumn('n_all_len',col_len('n_all'))\n",
    "\n",
    "for i in range(2,5):\n",
    "    df_utters = df_utters.withColumn('n'+ str(i) + '_len',col_len('n'+ str(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_utters.where(df_utters['n_all_len'] <= 14).select('n_all_len').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "# __________________Load DF of Hateful terms\n",
    "df_hate_words = pd.read_csv('data/raw_other/hateful_ngrams.csv')\n",
    "df_hate_words.set_index('ngram',drop=True,inplace=True)\n",
    "dict_hateweights = df_hate_words['prophate'].to_dict()\n",
    "hate_list = list(df_hate_words.index)\n",
    "\n",
    "df_utters = df_utters.na.drop(\"all\")\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"n_all\", outputCol=\"features\")\n",
    "\n",
    "cvec_model = cv.fit(df_utters)\n",
    "cvec_model.transform(df_utters).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['conv_id',  'timestamp', 'source', 'text']\n",
    "df_utters = dd.read_csv('out/article_data_df_1_1.ftr.csv',names=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_utters"
   ]
  },
  {
   "source": [
    "# Local run"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "import pandas as pd\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "import re\n",
    "#import apache_beam as beam\n",
    "\n",
    "import argparse\n",
    "#from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from nltk.stem.porter import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "import pyarrow\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "\n",
    "df_hate_words = pd.read_csv('data/raw_other/hateful_ngrams.csv')\n",
    "df_hate_words.set_index('ngram',drop=True,inplace=True)\n",
    "dict_hateweights = df_hate_words['prophate'].to_dict()\n",
    "hate_list = list(df_hate_words.index)\n",
    "\n",
    "\n",
    "def tokenize(tweet):\n",
    "    \"\"\"Removes punctuation & excess whitespace, sets to lowercase,\n",
    "    and stems tweets. Returns a list of stemmed tokens.\"\"\"\n",
    "    #tweet = \" \".join(re.split(\"[^a-zA-Z]*\", tweet.lower())).strip()\n",
    "    \n",
    "    tokens = re.split(\"[^a-zA-Z]*\", tweet.lower())\n",
    "    tokens = [stemmer.stem(t) for t in tweet.split()]\n",
    "    return tokens\n",
    "    \n",
    "def ngrams(tokens,n):\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    list_gram = [\" \".join(ngram) for ngram in ngrams]\n",
    "    return list_gram\n",
    "\n",
    "\n",
    "\n",
    "def process(self,element):\n",
    "    master = []\n",
    "    tokens = tokenize(element['text'])\n",
    "\n",
    "    for n in range(2,4):\n",
    "        current = ngrams(tokens,n)\n",
    "        master += current\n",
    "        \n",
    "\n",
    "    master += tokens\n",
    "\n",
    "    hate_score = 0\n",
    "    hate_hits = {}\n",
    "\n",
    "    for gram in master:\n",
    "        if gram in dict_hateweights.keys():\n",
    "            if gram in hate_hits:\n",
    "                hate_hits[gram] += 1 \n",
    "            else:\n",
    "                hate_hits[gram] = 1\n",
    "            hate_score += dict_hateweights[gram]\n",
    "    element['hate_score'] = hate_score\n",
    "    element['hate_hits'] = str(hate_hits)\n",
    "\n",
    "    return element\n",
    "\n",
    "df_utters = pd.read_parquet('data/combined_utterances.parquet')\n",
    "\n",
    "import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "df_utters_out = df_utters.progress_apply(lambda row:process(row),axis=1,result_type='expand')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "216px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}